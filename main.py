import pandas as pd
import os
import time
import logging
import io
import contextlib
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm

from src.solver import CoverageSolver
from src.merger import CandidateMerger
from src.config import *

# --- 1. ä»…å®šä¹‰å¸¸é‡ (ä¸è¦åœ¨è¿™é‡Œåšæ–‡ä»¶æ“ä½œï¼) ---
LOG_DIR = "./logs"
LOG_FILE = os.path.join(LOG_DIR, "solver_run.log")


def process_single_city_pipeline(city_name, city_df, max_capacity):
    """
    å•ä¸ªåŸå¸‚çš„å®Œæ•´å¤„ç†æµæ°´çº¿ã€‚
    æ³¨æ„ï¼šå­è¿›ç¨‹ä¸­ä¸è¦é…ç½® logging åˆ°æ–‡ä»¶ï¼Œåªæ•è· stdoutã€‚
    """
    # åˆ›å»ºå­—ç¬¦ä¸²ç¼“å†²åŒºæ•è·è¾“å‡º
    log_capture = io.StringIO()

    centers = []
    details = pd.DataFrame()
    status = "Fail"
    n_centers = 0

    # ã€æ–°å¢ã€‘ç»Ÿè®¡å˜é‡
    count_120 = 0
    count_50 = 0
    count_merged = 0

    # é‡å®šå‘ stdout åˆ°ç¼“å†²åŒº
    with contextlib.redirect_stdout(log_capture):
        try:
            print(f"[{time.strftime('%H:%M:%S')}] === å¼€å§‹å¤„ç†: {city_name} (Rows: {len(city_df)}) ===")

            # 1. é˜¶æ®µä¸€ï¼šCapacity=50
            solver_50 = CoverageSolver(city_df, max_capacity=50)
            solver_50.solve(use_existing_init=False)
            c50 = solver_50.final_centers
            count_50 = len(c50)  # è®°å½•æ•°é‡

            # 2. é˜¶æ®µäºŒï¼šCapacity=120
            solver_120 = CoverageSolver(city_df, max_capacity=120)
            solver_120.solve(use_existing_init=False)
            c120 = solver_120.final_centers
            count_120 = len(c120)  # è®°å½•æ•°é‡

            # 3. é˜¶æ®µä¸‰ï¼šèåˆ
            merged = CandidateMerger.merge_and_prune(c120, c50, distance_threshold_km=MERGE_DISTANCE_THRESHOLD)
            count_merged = len(merged)  # è®°å½•æ•°é‡

            # 4. é˜¶æ®µå››ï¼šæœ€ç»ˆä¼˜åŒ–
            final_solver = CoverageSolver(city_df, max_capacity=max_capacity)
            final_solver.load_external_candidates(merged)
            centers, details = final_solver.solve(use_existing_init=True)

            n_centers = len(centers)
            status = "Success"
            print(f"[{time.strftime('%H:%M:%S')}] === å¤„ç†å®Œæˆ: {city_name} | æœ€ç»ˆç«™ç‚¹: {n_centers} ===")
            print("-" * 30)

        except Exception as e:
            status = "Error"
            print(f"âŒ å¼‚å¸¸: {str(e)}")
            import traceback
            traceback.print_exc()

    # è¿”å›æ‰€æœ‰ç»“æœï¼ŒåŒ…æ‹¬æ–°å¢çš„ç»Ÿè®¡æ•°æ®
    return city_name, centers, details, status, n_centers, count_120, count_50, count_merged, log_capture.getvalue()


def main():
    # æ–‡ä»¶æ“ä½œå’Œæ—¥å¿—é…ç½®ç§»å…¥ main å‡½æ•°
    if not os.path.exists(LOG_DIR):
        os.makedirs(LOG_DIR)

    if os.path.exists(LOG_FILE):
        try:
            os.remove(LOG_FILE)
        except PermissionError:
            print("âš ï¸ è­¦å‘Š: æ— æ³•åˆ é™¤æ—§æ—¥å¿—æ–‡ä»¶(å¯èƒ½è¢«å ç”¨)ï¼Œå°†è¿½åŠ å†™å…¥ã€‚")

    # é…ç½®ä¸»è¿›ç¨‹æ—¥å¿— (åªæœ‰ä¸»è¿›ç¨‹å†™æ–‡ä»¶)
    logging.basicConfig(
        filename=LOG_FILE,
        level=logging.INFO,
        format="%(message)s",
        encoding='utf-8'
    )

    start_time = time.time()
    print(f"ğŸš€ å¯åŠ¨æ™ºèƒ½é€‰å€ç³»ç»Ÿ...")
    print(f"ğŸ“‚ æ—¥å¿—å­˜æ”¾äº: {LOG_FILE}")

    # 1. è¯»å–å…¨é‡æ•°æ®
    if not os.path.exists(DATA_PATH):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {DATA_PATH}")
        return

    print(f"â³ æ­£åœ¨è¯»å–æ•°æ®...")
    try:
        df = pd.read_csv(DATA_PATH, sep='\t')
        if 'tbsg_latitude' not in df.columns:
            df = pd.read_csv(DATA_PATH, sep=',')
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {e}")
        return

    # 2. æŒ‰åŸå¸‚æ‹†åˆ†æ•°æ®
    unique_cities = df[COL_CITY].unique()
    city_tasks = []
    for city in unique_cities:
        if pd.isna(city) or str(city).strip() == "": continue
        sub_df = df[df[COL_CITY] == city].copy()
        if len(sub_df) < 1: continue
        city_tasks.append((city, sub_df))

    total_cities = len(city_tasks)
    print(f"âœ… æ•°æ®åŠ è½½å®Œæ¯•ï¼Œå…± {total_cities} ä¸ªåŸå¸‚ä»»åŠ¡ã€‚")

    # 3. å¹¶è¡Œæ‰§è¡Œ
    # Windowsä¸‹å»ºè®®ä¸è¦å æ»¡æ‰€æœ‰CPUï¼Œç•™1-2ä¸ªæ ¸ç»™ç³»ç»Ÿ
    max_workers = max(1, min(os.cpu_count() - 2, 20))
    print(f"ğŸ”¥ å¯åŠ¨è¿›ç¨‹æ±  (Workers={max_workers})...")

    all_centers = []
    all_details = []
    success_count = 0
    error_count = 0

    # ã€æ–°å¢ã€‘å…¨å±€ç»Ÿè®¡å˜é‡
    total_c120 = 0
    total_c50 = 0
    total_merged = 0
    total_final = 0

    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(process_single_city_pipeline, city, data, 120): city
            for city, data in city_tasks
        }

        # è¿›åº¦æ¡
        pbar = tqdm(as_completed(futures), total=total_cities, unit="city", ncols=100)

        for future in pbar:
            try:
                # è·å–å­è¿›ç¨‹è¿”å›çš„ç»“æœ (è§£åŒ…æ–°å¢çš„å˜é‡)
                city_name, centers, details, status, n_centers, c120_n, c50_n, merged_n, log_str = future.result()

                # 1. å°†å­è¿›ç¨‹çš„æ—¥å¿—å†™å…¥ä¸»æ—¥å¿—æ–‡ä»¶
                logging.info(log_str)

                # 2. å¤„ç†ä¸šåŠ¡æ•°æ®
                if status == "Success":
                    all_centers.append(centers)
                    all_details.append(details)
                    success_count += 1

                    # ç´¯åŠ ç»Ÿè®¡
                    total_c120 += c120_n
                    total_c50 += c50_n
                    total_merged += merged_n
                    total_final += n_centers

                    pbar.set_postfix_str(f"Last: {city_name} ({n_centers}ç«™) | Err: {error_count}")
                else:
                    error_count += 1
                    pbar.set_postfix_str(f"Last: {city_name} [ERR] | Err: {error_count}")
            except Exception as e:
                error_count += 1
                print(f"\nâŒ ä¸»è¿›ç¨‹å¤„ç†ç»“æœæ—¶å¼‚å¸¸: {e}")

    # 4. åˆå¹¶ç»“æœ
    print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ...")
    if all_centers:
        final_centers_df = pd.concat(all_centers, ignore_index=True)
        final_details_df = pd.concat(all_details, ignore_index=True)

        # ç”Ÿæˆå…¨å±€å”¯ä¸€ID
        final_centers_df['center_id'] = [f"C_{i + 1:06d}" for i in range(len(final_centers_df))]

        final_centers_df.to_csv(OUTPUT_CENTERS, index=False, encoding='utf-8-sig')
        final_details_df.to_csv(OUTPUT_DETAILS, index=False, encoding='utf-8-sig')

        duration = time.time() - start_time

        # ã€æ–°å¢ã€‘è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        total_initial = total_c120 + total_c50
        reduced_by_merge = total_initial - total_merged  # èåˆé˜¶æ®µå‰”é™¤
        reduced_by_opt = total_merged - total_final  # æœ€ç»ˆä¼˜åŒ–å‰”é™¤
        total_reduced = total_initial - total_final  # æ€»å‰”é™¤

        print("-" * 50)
        print(f"âœ… å…¨éƒ¨å®Œæˆï¼")
        print(f"â±ï¸ æ€»è€—æ—¶: {duration:.1f}s")
        print(f"ğŸ™ï¸ æˆåŠŸ: {success_count} | å¤±è´¥: {error_count}")
        print(f"ğŸ“ æ€»ç«™ç‚¹æ•°: {len(final_centers_df)}")
        print(f"ğŸ“‚ ç»“æœä¿å­˜: data/")
        print(f"ğŸ“ è¿è¡Œæ—¥å¿—: {LOG_FILE}")

        print("-" * 50)
        print(f"ğŸ“Š ç®—æ³•ä¼˜åŒ–ç»Ÿè®¡æŠ¥è¡¨:")
        print(f"   1. åˆå§‹ç”Ÿæˆæ± :")
        print(f"      - C120 (å¤§ç«™): {total_c120}")
        print(f"      - C50  (å°ç«™): {total_c50}")
        print(f"      - åˆè®¡åˆå§‹ç‚¹ : {total_initial}")
        print(f"   2. èåˆä¸ä¼˜åŒ–:")
        print(f"      - èåˆåå€™é€‰ç‚¹: {total_merged} (ğŸ“‰ èåˆå‰”é™¤: {reduced_by_merge})")
        print(f"      - æœ€ç»ˆä¼˜é€‰ç«™ç‚¹: {total_final}  (ğŸ“‰ ä¼˜åŒ–å‰”é™¤: {reduced_by_opt})")
        print(f"   3. æ€»ä½“æ•ˆæœ:")
        print(f"      - æ€»è®¡å‡å°‘å†—ä½™ç‚¹ä½: {total_reduced}")
        print("-" * 50)
    else:
        print("âŒ æœªç”Ÿæˆä»»ä½•ç»“æœï¼Œè¯·æ£€æŸ¥ logs/solver_run.log")


if __name__ == "__main__":
    main()
